#!/usr/bin/env python3
#this code was generated by an LLM and proof-read, edited and iterated upon by Jonas Korkosh

import os
import glob
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import argparse
import re
import ast
from itertools import product


def read_progress_csv(filepath):
    """
    Reads a single progress.csv file, returning a list of mean batch rewards.
    Skips any lines starting with '#', which contain metadata.
    """
    df = pd.read_csv(filepath, comment="#")
    rewards = df["ep_reward_mean"].tolist()
    return rewards

def collect_all_progress_files(directory="."):
    """
    Collects all progress.csv files in the given directory and its subdirectories recursively.
    Returns a list of file paths.
    """
    pattern = os.path.join(directory, "**", "*progress.csv")
    return glob.glob(pattern, recursive=True)

def compute_reward_statistics(reward_arrays):
    """
    Given a list of lists (reward_arrays), where each list contains per-batch reward data
    from one run, compute arrays for:
      - batch indices (x-axis)
      - mean reward
      - standard deviation

    Returns: (batches, means, stds)
    """
    max_len = max(len(arr) for arr in reward_arrays)
    means = []
    stds = []

    for batch_idx in range(max_len):
        batch_rewards = []
        for arr in reward_arrays:
            if batch_idx < len(arr):
                batch_rewards.append(arr[batch_idx])
        if batch_rewards:
            means.append(np.mean(batch_rewards))
            stds.append(np.std(batch_rewards))
        else:
            break

    batches = np.arange(1, len(means) + 1)
    return batches, means, stds

def plot_multiple_configs(dir_label_batches_pairs, append=False, title=None):
    """
    For each (directory, label) pair:
      1) Collect all progress.csv files
      2) Read the reward data
      3) Compute batches/means/stds
      4) Plot them with a given label
    All on the same figure.
    """
    plt.figure(figsize=(8, 5))
    last_x = 0
    for directory, label, num_batches in dir_label_batches_pairs:
        file_paths = collect_all_progress_files(directory)
        if not file_paths:
            print(f"No progress.csv files found in '{directory}', skipping.")
            continue

        # Gather all runs in this directory into a list of reward arrays
        reward_runs = [read_progress_csv(fp) for fp in file_paths]

        # Compute the statistics
        batches, means, stds = compute_reward_statistics(reward_runs)
        if num_batches:
            batches = batches[:num_batches]
            means = means[:num_batches]
            stds = stds[:num_batches]
        if append:
            batches = np.array(batches) + last_x
        # Plot on the same figure
        plt.plot(batches, means, label=label)
        plt.fill_between(
            batches,
            np.array(means) - np.array(stds),
            np.array(means) + np.array(stds),
            alpha=0.2
        )

        last_x = batches[-1]

    plt.title(title)
    plt.xlabel("Batch Index")
    plt.ylabel("Reward")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def generate_default_label(dirpath):
    """
    Attempt to parse a directory path of the form:
        /.../logs/hardware/QubeSwingupEnv/1e6/seed-984
    Returning a label like:
        hardware/QubeSwingupEnv/1e6/seed-984
    Falls back to "UnknownRun" if parsing fails.
    """
    label = "UnknownRun"
    parts = dirpath.split("/")
    try:
        logs_idx = parts.index("logs")
        # everything that comes after "logs" is joined into the label
        label = "/".join(parts[logs_idx + 1 :])
    except (ValueError, IndexError):
        pass
    return label

def parse_reward_txt(filepath):
    """
    Parses a reward.txt file to extract the list of 'real_rollouts_rewards'.
    Returns a list of floats. Returns an empty list if not found or error.
    """
    rewards_list = []
    with open(filepath, 'r') as f:
        for line in f:
            if line.startswith("real_rollouts_rewards:"):
                list_str = line.split(":", 1)[1].strip()
                parsed_list = ast.literal_eval(list_str)
                # Ensure it's a list and all elements are numbers
                if isinstance(parsed_list, list) and all(isinstance(r, float) for r in parsed_list):
                    rewards_list = parsed_list
                else:
                    print(f"Warning: Parsed data from 'real_rollouts_rewards' in {filepath} is not a list of numbers. Content: {list_str}")
                break 
    return rewards_list

def parse_angles_txt(filepath):
    """
    Parses an angles.txt file to extract all episode trajectories.
    Each trajectory is a list of states [theta, alpha, theta_dot, alpha_dot].
    Returns a list of trajectories.
    """
    trajectories = []
    with open(filepath, 'r') as f:
        for line in f:
            if line.startswith("episode_"):
                try:
                    list_str = line.split(":", 1)[1].strip()
                    parsed_list = ast.literal_eval(list_str)
                    if isinstance(parsed_list, list):
                        trajectories.append(np.array(parsed_list))
                except (ValueError, SyntaxError) as e:
                    print(f"Warning: Could not parse trajectory in {filepath}. Error: {e}")
    return trajectories

def collect_simopt_iteration_data(seed_dirs, iter_num, num_batches_limit_per_iter):
    """
    Collects data for a specific SimOpt iteration from multiple seed directories.
    Applies num_batches_limit_per_iter to each progress.csv data.
    returns:
        - all_progress_rewards_for_iter: List of lists of rewards from progress.csv from each seed [[progress_rewards_seed1_iter_num], [progress_rewards_seed2_iter_num], ...]
        - all_end_of_iter_rollout_rewards: List of rewards from reward.txt from each seed:  [[rewardstxt_list_seed1_iternum], [rewardstxt_list_seed2_iter_num], ...]
    """
    all_progress_rewards_for_iter = []
    all_end_of_iter_rollout_rewards = []

    for seed_dir in seed_dirs:
        iter_dir = os.path.join(seed_dir, f"iter-{iter_num}")
        
        progress_csv_path = os.path.join(iter_dir, "progress.csv")
        if os.path.exists(progress_csv_path):
            rewards = read_progress_csv(progress_csv_path)
            if rewards:
                if num_batches_limit_per_iter and num_batches_limit_per_iter > 0:
                    rewards = rewards[:num_batches_limit_per_iter]
                if rewards: # Check again after truncation
                    all_progress_rewards_for_iter.append(rewards)
        else:
            print(f"Info: progress.csv not found in {iter_dir}")

        reward_txt_path = os.path.join(iter_dir, "reward.txt")
        rewards = parse_reward_txt(reward_txt_path) # std_r from reward.txt not directly used for this plot
        if rewards:
            all_end_of_iter_rollout_rewards += rewards #list concatenation
        elif os.path.exists(iter_dir): # Only warn if iter_dir exists but reward.txt is bad/missing
            print(f"Info: mean_reward not found or invalid in {reward_txt_path}")
            
    return {
        "all_progress_rewards_for_iter": all_progress_rewards_for_iter,
        "all_end_of_iter_rollout_rewards": all_end_of_iter_rollout_rewards,
    }

def plot_simopt_experiment(seed_directories, num_simopt_iterations_to_plot, title, num_batches_limit_per_iteration):
    """
    Plots SimOpt experiment results, appending iterations and showing mean/std across seeds.
    """
    plt.figure(figsize=(12, 7))
    current_total_batches_offset = 0
    
    # colors = plt.cm.viridis(np.linspace(0, 0.9, num_simopt_iterations_to_plot))
    prop_cycle = plt.rcParams['axes.prop_cycle']
    colors = [prop_cycle.by_key()['color'][i % len(prop_cycle.by_key()['color'])] for i in range(num_simopt_iterations_to_plot)]

    for i in range(num_simopt_iterations_to_plot):
        print(f"Processing SimOpt iteration {i}...")
        iteration_data = collect_simopt_iteration_data(seed_directories, i, num_batches_limit_per_iteration)
        
        progress_rewards_lists = iteration_data["all_progress_rewards_for_iter"]
        all_end_of_iter_rollout_rewards = iteration_data["all_end_of_iter_rollout_rewards"]

        plot_label_suffix = f"(Iter {i}, {len(progress_rewards_lists)} seeds)" if progress_rewards_lists else f"(Iter {i}, No progress data)"

        if not progress_rewards_lists:
            print(f"  No progress.csv data found for iteration {i} across any seeds. Skipping batch plot segment.")
            # Attempt to plot end-of-iteration point if data exists, but x-pos might be an issue.
            # For now, if no batch data, we can't reliably place the end-of-iter point.
            # A placeholder for x could be added if needed, e.g., current_total_batches_offset += some_default_length
            continue

        batches_iter_segment, means_iter_segment, stds_iter_segment = compute_reward_statistics(progress_rewards_lists)

        if not batches_iter_segment.size:
            print(f"  No valid batch data after statistics for iteration {i}. Skipping batch plot segment.")
            continue

        # Shift x-axis for appending this iteration's segment
        plot_batches_segment = np.array(batches_iter_segment) + current_total_batches_offset
        
        line_color = colors[i % len(colors)]
        
        # Plot mean line and std fill for batch rewards
        plt.plot(plot_batches_segment, means_iter_segment, color=line_color, label=f"Iter {i} Batch Rewards")
        plt.fill_between(
            plot_batches_segment,
            np.array(means_iter_segment) - np.array(stds_iter_segment),
            np.array(means_iter_segment) + np.array(stds_iter_segment),
            alpha=0.2,
            color=line_color
        )
        
        iter_segment_end_x = plot_batches_segment[-1]

        # Plot end-of-iteration aggregate reward point (mean of means from reward.txt, with std of these means)
        if all_end_of_iter_rollout_rewards:
            # Mean of the reported mean_rewards from reward.txt across seeds
            aggregate_mean_at_iter_end = np.mean(all_end_of_iter_rollout_rewards)
            # Std of the reported mean_rewards from reward.txt across seeds
            aggregate_std_at_iter_end = np.std(all_end_of_iter_rollout_rewards)
            
            plt.errorbar(
                x=iter_segment_end_x, 
                y=aggregate_mean_at_iter_end,
                yerr=aggregate_std_at_iter_end,
                fmt='o', 
                color=line_color, 
                capsize=5,
                markersize=8,
                markeredgecolor='black',
                elinewidth=2,
                label=f"Iter {i} Final Reward" # One label per iteration for its final reward point
            )
            print(f"  Iter {i}: Plotted end-of-iteration avg reward {aggregate_mean_at_iter_end:.2f} +/- {aggregate_std_at_iter_end:.2f} at batch {iter_segment_end_x}")
        else:
            print(f"  No reward.txt data found/parsed for iteration {i} to plot aggregate end-of-iteration point.")

        current_total_batches_offset = iter_segment_end_x 

    plt.title(title if title else "SimOpt Experiment: Mean Rewards vs. Batches")
    plt.xlabel("Total Batch Index (Appended Iterations)")
    plt.ylabel("Mean Episode Reward")
    plt.legend(loc='best')
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def plot_evaluation_results(dir_label_reward_pairs, title):
    """
    Plots training curve and final evaluation reward for a set of models.
    """
    plt.figure(figsize=(12, 7))
    current_total_batches_offset = 0

    prop_cycle = plt.rcParams['axes.prop_cycle']
    colors = [prop_cycle.by_key()['color'][i % len(prop_cycle.by_key()['color'])] for i in range(len(dir_label_reward_pairs))]

    for i, (directory, label, reward_type, num_batches) in enumerate(dir_label_reward_pairs):
        line_color = colors[i % len(colors)]
        # 1. Plot the training curve from progress.csv
        progress_paths = collect_all_progress_files(directory)
        if progress_paths:
            reward_runs = [read_progress_csv(fp) for fp in progress_paths]
            batches, means, stds = compute_reward_statistics(reward_runs)

            if num_batches: # A value of 0 or None will be Falsy
                batches = batches[:num_batches]
                means = means[:num_batches]
                stds = stds[:num_batches]

            plot_batches = batches + current_total_batches_offset
            plt.plot(plot_batches, means, label=f"{label} Training", color=line_color)
            plt.fill_between(
                plot_batches,
                np.array(means) - np.array(stds),
                np.array(means) + np.array(stds),
                alpha=0.2,
                color=line_color
            )
            current_total_batches_offset = plot_batches[-1] if batches.size > 0 else current_total_batches_offset
        else:
            print(f"No progress.csv found for {label} in {directory}")

        # 2. Plot the final evaluation from reward.txt
        if reward_type:
            reward_filename = f"reward_{reward_type}.txt"
        else:
            reward_filename = "reward.txt"
        reward_paths = glob.glob(os.path.join(directory, "**", reward_filename), recursive=True)
        
        if reward_paths:
            all_eval_rewards = []
            for path in reward_paths:
                rewards_from_file = parse_reward_txt(path)
                if rewards_from_file:
                    all_eval_rewards.extend(rewards_from_file)
            
            if all_eval_rewards:
                mean_eval_reward = np.mean(all_eval_rewards)
                std_eval_reward = np.std(all_eval_rewards)

                plt.errorbar(
                    x=current_total_batches_offset,
                    y=mean_eval_reward,
                    yerr=std_eval_reward,
                    fmt='o',
                    capsize=5,
                    markersize=8,
                    markeredgecolor='black',
                    elinewidth=2,
                    label=f"{label} Final Reward ({reward_type if reward_type else 'default'})",
                    color=line_color
                )

    plt.title(title if title else "Model Comparison")
    plt.xlabel("Total Batch Index")
    plt.ylabel("Mean Episode Reward")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.show()

def plot_reward_histogram(directories, title, reward_suffix, bins=40, y_lim=None):
    """
    Finds all specified reward files in the given directories and plots a histogram
    of the collected rewards.
    """
    all_rewards = []
    
    if reward_suffix:
        reward_filename = f"reward_{reward_suffix}.txt"
    else:
        reward_filename = "reward.txt"
        
    print(f"Searching for '{reward_filename}' files...")

    for directory in directories:
        reward_paths = glob.glob(os.path.join(directory, "**", reward_filename), recursive=True)
        if not reward_paths:
            print(f"Warning: No '{reward_filename}' files found in {directory}")
            continue
            
        for path in reward_paths:
            all_rewards.extend(parse_reward_txt(path))

    if not all_rewards:
        print("No reward data found to plot. Exiting.")
        return

    plt.figure(figsize=(10, 6))
    plt.hist(all_rewards, bins=bins, edgecolor='black', alpha=0.7)
    
    mean_reward = np.mean(all_rewards)
    std_reward = np.std(all_rewards)
    
    plt.axvline(mean_reward, color='r', linestyle='dashed', linewidth=2, label=f'Mean: {mean_reward:.2f}')
    
    plot_title = title if title else "Distribution of Rewards"
    plt.title(plot_title)
    plt.xlabel("Episode Reward")
    plt.ylabel("Frequency")
    if y_lim:
        plt.ylim(y_lim)
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    print(f"Generated histogram from {len(all_rewards)} episodes.")
    print(f"Mean: {mean_reward:.2f}, Std Dev: {std_reward:.2f}")
    plt.show()

def plot_angle_trajectories(directory, title, angle_suffix):
    """
    Finds all specified angles files in the given directory, parses them,
    and plots the trajectories.
    """
    if angle_suffix:
        filename = f"angles_{angle_suffix}.txt"
    else:
        filename = "angles.txt"

    angle_paths = glob.glob(os.path.join(directory, "**", filename), recursive=True)
    if not angle_paths:
        print(f"No '{filename}' files found in {directory}")
        return

    all_trajectories = []
    for path in angle_paths:
        all_trajectories.extend(parse_angles_txt(path))
    
    if not all_trajectories:
        print("No angle data found to plot.")
        return

    fig, axes = plt.subplots(4, 1, figsize=(10, 15), sharex=True)
    angle_names = ['Theta (rad)', 'Alpha (rad)', 'Theta_dot (rad/s)', 'Alpha_dot (rad/s)']

    for i in range(4): # For each state variable
        ax = axes[i]
        for j, traj in enumerate(all_trajectories):
            if j > 2:
                break
            ax.plot(traj[:, i], alpha=0.5)
        ax.set_ylabel(angle_names[i])
        ax.grid(True)
    
    axes[-1].set_xlabel("Time Step")
    fig.suptitle(title if title else f"Angle Trajectories from {directory}")
    plt.tight_layout(rect=[0, 0.03, 1, 0.96])
    plt.show()


def plot_reward_histogram_by_sign(directories, title, suffix, bins, state_vars):
    """
    Plots multiple reward histograms based on the sign combinations of initial state variables.
    """
    state_map = {'theta': 0, 'alpha': 1, 'theta_dot': 2, 'alpha_dot': 3}
    
    for var in state_vars:
        if var not in state_map:
            print(f"Error: Invalid state_var '{var}'. Must be one of {list(state_map.keys())}")
            return
    
    state_indices = [state_map[var] for var in state_vars]

    # --- NEW: Data Aggregation Loop ---
    all_trajectories = []
    all_rewards = []
    for directory in directories:
        angle_filename = f"angles_{suffix}.txt" if suffix else "angles.txt"
        reward_filename = f"reward_{suffix}.txt" if suffix else "reward.txt"

        angle_paths = glob.glob(os.path.join(directory, "**", angle_filename), recursive=True)
        reward_paths = glob.glob(os.path.join(directory, "**", reward_filename), recursive=True)
        
        # This assumes a 1-to-1 mapping of angle files to reward files, which is reasonable.
        for angle_path in angle_paths:
            # Try to find a corresponding reward file in the same sub-directory
            reward_path = angle_path.replace(angle_filename, reward_filename)
            if os.path.exists(reward_path):
                trajectories = parse_angles_txt(angle_path)
                rewards = parse_reward_txt(reward_path)
                if len(trajectories) == len(rewards):
                    all_trajectories.extend(trajectories)
                    all_rewards.extend(rewards)
                else:
                    print(f"Warning: Mismatch in {angle_path} and {reward_path}. Skipping.")
            else:
                 print(f"Warning: Could not find corresponding reward file for {angle_path}. Skipping.")
    # --- END NEW ---

    if not all_trajectories:
        print("No valid trajectory/reward pairs found to plot.")
        return

    sign_combinations = list(product([1, -1], repeat=len(state_vars)))
    reward_groups = {combo: [] for combo in sign_combinations}

    for traj, reward in zip(all_trajectories, all_rewards):
        initial_signs = tuple(np.sign(traj[0][i]) if traj[0][i] != 0 else 1 for i in state_indices)
        if initial_signs in reward_groups:
            reward_groups[initial_signs].append(reward)

    num_combos = len(sign_combinations)
    if num_combos <= 2:
        nrows, ncols = 1, 2
    elif num_combos <= 4:
        nrows, ncols = 2, 2
    else:
        nrows, ncols = 2, 4
    
    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols*4, nrows*3.5), sharex=True, sharey=True)
    axes = axes.flatten()
    bin_count = bins if bins and bins > 0 else 'auto'
    
    x_min, x_max = np.min(all_rewards), np.max(all_rewards)

    for i, combo in enumerate(sign_combinations):
        ax = axes[i]
        rewards_list = reward_groups[combo]
        
        label_parts = []
        for var_name, sign in zip(state_vars, combo):
            sign_str = '>=' if sign == 1 else '<'
            label_parts.append(f'{var_name} {sign_str} 0')
        
        ax.hist(rewards_list, bins=bin_count, alpha=0.7, range=(x_min, x_max))
        ax.set_title(', '.join(label_parts) + f' (N={len(rewards_list)})')
        ax.grid(True)

    for i in range(num_combos, len(axes)):
        axes[i].set_visible(False)

    fig.suptitle(title if title else f"Reward Distribution by Initial Signs of {', '.join(state_vars)}")
    fig.supxlabel("Episode Reward")
    fig.supylabel("Frequency")
    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show()

def main():
    parser = argparse.ArgumentParser(description="Plot rewards from monitor.csv files.")
    parser.add_argument(
        "-d",
        "--directories",
        type=str,
        nargs="+",
        default=["."],
        help="List of directories in which to search for progress.csv files."
    )
    parser.add_argument(
        "-l",
        "--labels",
        type=str,
        nargs="+",
        default=None,
        help="List of labels corresponding to each directory."
    )
    parser.add_argument(
        "-nb",
        "--num-batches",
        type=int,
        nargs="+",
        default=None,
        help="Number of batches to plot for each corresponding directory. If 0, all batches will be plotted."
    )
    parser.add_argument(
        "-a",
        "--append",
        action="store_true",
        default=False,
        help="Append to the end of the existing plot instead of starting at x=0. Runs in the order specified in the directories argument."
    )
    parser.add_argument(
        "-t",
        "--title",
        type=str,
        default="Rewards vs. Batches",
        help="Title of the plot."
    )
    parser.add_argument(
        "-so",
        "--simopt",
        action="store_true",
        default=False,
        help="Use the simopt directory structure to plot the data. Plots iterations consequtively. Looks for reward.txt at the end of each iteration."
    )
    parser.add_argument(
        "-soi",
        "--simopt-iters",
        type=int,
    )
    parser.add_argument(
        "--eval",
        action="store_true",
        help="Plot training curve and final evaluation point for specified models."
    )
    parser.add_argument(
        "--reward-types",
        type=str,
        nargs='+',
        default=None,
        help="The type of reward file to use for plotting (e.g., 'sim' for 'reward_sim.txt')."
    )
    parser.add_argument(
        "--plot-step-rewards",
        action="store_true",
        default=False,
        help="Also plot per_step_rewards from the reward.txt file if available."
    )
    parser.add_argument(
        "--histogram",
        action="store_true",
        help="Plot a histogram of rewards from reward.txt files."
    )
    parser.add_argument(
        "--bins",
        type=int,
        default=0,
        help="Number of bins for the histogram plot."
    )
    parser.add_argument(
        "--y-lim",
        type=int,
        nargs=2,
        default=None,
        help="Set the y-axis limits for the plot. Provide two integers: min and max."
    )
    parser.add_argument(
        "--plot-angles",
        action="store_true",
        help="Plot angle trajectories from angles.txt files."
    )
    parser.add_argument(
        "--angle-suffix",
        type=str,
        default=None,
        help="Suffix for the angles file (e.g., 'sim' for 'angles_sim.txt')."
    )
    parser.add_argument(
        "--split-histogram-by-sign",
        type=str,
        nargs='+', # Accept one or more values
        default=None,
        choices=['theta', 'alpha', 'theta_dot', 'alpha_dot'],
        help="Plot a split histogram based on the sign of one or more initial state variables."
    )
    args = parser.parse_args()

    if args.split_histogram_by_sign:
        reward_suffix = args.reward_types[0] if args.reward_types else None
        plot_reward_histogram_by_sign(args.directories, args.title, reward_suffix, args.bins, args.split_histogram_by_sign)

    elif args.plot_angles:
        if len(args.directories) > 1:
            print("Warning: Plotting angles for multiple directories may be cluttered. Using the first directory provided.")
        plot_angle_trajectories(args.directories[0], args.title, args.angle_suffix)

    elif args.histogram:
        if args.directories == ["."]:
            print("Error: For --histogram mode, please provide specific directories via -d.")
            return
        reward_suffix = args.reward_types[0] if args.reward_types else None
        plot_reward_histogram(args.directories, args.title, reward_suffix, args.bins, args.y_lim)
    
    elif args.eval:
        if args.directories == ["."]:
            print("Error: For --eval mode, please provide specific directories via -d.")
            return
        if not args.labels or len(args.labels) != len(args.directories):
            print("Error: For --eval mode, you must provide a label for each directory via -l.")
            return
        
        num_batches_list = []
        if args.num_batches:
            if len(args.num_batches) == len(args.directories):
                num_batches_list = args.num_batches
            else:
                 print(f"Warning: Number of batch limits ({len(args.num_batches)}) does not match number of directories ({len(args.directories)}). Using provided values and defaulting rest to 0 (all batches).")
                 num_batches_list = args.num_batches + [0] * (len(args.directories) - len(args.num_batches))
        else:
            num_batches_list = [0] * len(args.directories) # Default to 0 (all batches)

        #Logic for handling multiple reward types (what system the reward is from eg. sim, real, sim_double_mass, etc.)
        reward_types = []
        if args.reward_types:
            if len(args.reward_types) == len(args.directories):
                reward_types = args.reward_types
            else:
                print(f"Warning: Number of reward types ({len(args.reward_types)}) does not match number of directories ({len(args.directories)}). Using provided types and defaulting rest to 'reward.txt'.")
                reward_types = args.reward_types + [None] * (len(args.directories) - len(args.reward_types))
        else:
            # Default to None for all if --reward-types is not used
            reward_types = [None] * len(args.directories)

        dir_label_reward_batch_pairs = list(zip(args.directories, args.labels, reward_types, num_batches_list))
        plot_evaluation_results(dir_label_reward_batch_pairs, args.title)
        
    elif args.simopt:
        if args.directories == ["."]:
            print("Error: For --simopt mode, please provide specific seed directories via -d or --directories (e.g., path/to/seed-123 path/to/seed-456).")
            return
        
        simopt_max_batches_per_iter = 0 
        if args.num_batches and args.num_batches[0] > 0: # Check if list exists and first element is > 0
            simopt_max_batches_per_iter = args.num_batches[0]
            print(f"SimOpt mode: Limiting batches per iteration segment to {simopt_max_batches_per_iter}.")
        elif args.num_batches and args.num_batches[0] <=0 : # Check if 0 or negative
             print(f"SimOpt mode: Plotting all available batches per iteration segment (--num-batches set to {args.num_batches[0]}).")
        else: # args.num_batches is None
             print(f"SimOpt mode: Plotting all available batches per iteration segment (default --num-batches).")

        
        plot_simopt_experiment(
            seed_directories=args.directories,
            num_simopt_iterations_to_plot=args.simopt_iters,
            title=args.title, 
            num_batches_limit_per_iteration=simopt_max_batches_per_iter
        )
    else:
        if args.labels is None or len(args.labels) != len(args.directories):
            # Create default labels if not provided or mismatch in length
            labels = [generate_default_label(dirpath) for dirpath in args.directories]
        else:
            labels = args.labels
        if args.num_batches is None or len(args.num_batches) != len(args.directories):
            # Create default num_batches if not provided or mismatch in length
            print("Warning: num_batches length does not match number of directories, setting all to 0")
            num_batches = [0 for _ in range(len(args.directories))]
        else:
            num_batches = args.num_batches

        dir_label_batches_pairs = list(zip(args.directories, labels, num_batches))
        plot_multiple_configs(dir_label_batches_pairs, append=args.append, title=args.title)

if __name__ == "__main__":
    main()

"""
Example commands:
python jonas_plot.py --simopt --simopt-iters 4 -d /home/jonas/Masteroppgave/qube-baselines/logs/SimOpt/QubeSwingupEnv/sim2sim_double_mass_12124545/seed-344 /home/jonas/Masteroppgave/qube-baselines/logs/SimOpt/QubeSwingupEnv/sim2sim_double_mass_12124545/seed-781 /home/jonas/Masteroppgave/qube-baselines/logs/SimOpt/QubeSwingupEnv/sim2sim_double_mass_12124545/seed-414 /home/jonas/Masteroppgave/qube-baselines/logs/SimOpt/QubeSwingupEnv/sim2sim_double_mass_12124545/seed-560 --title "SimOpt test - sim2sim with double mass, N=4"

python jonas_plot.py --eval \
-d logs/simulator/QubeSwingupEnv/3e6/ logs/simulator/QubeSwingupEnv/1e6/ \
-l "Sim-Only" "Finetuned" \
-t "Comparison of Sim-Only vs. Finetuning on Real Pendulum"

python jonas_plot.py --eval \
-d logs/simulator/QubeSwingupEnv/3e6/ logs/simulator/QubeSwingupEnv/1e6/ \
-l "Sim-Only" "Finetuned" \
-t "Comparison of Sim-Only vs. Finetuning on Sim With Double mp" \
--reward-types double_mp double_mp

python jonas_plot.py --histogram \
-d logs/simulator/QubeSwingupEnv/1e6/ \
--reward-types double_mp \
--y-lim 0 100 --bins 40 \
-t "Histogram of Rewards from Sim-Only QubeSwingupEnv Deployed on Double mp Sim"

python jonas_plot.py --split-histogram-by-sign alpha theta alpha_dot theta_dot \
-d logs/simulator/QubeSwingupEnv/3e6/ \
--reward-types double_mp --bins 40 \
-t "Histogram of Rewards from Sim-Only QubeSwingupEnv Deployed on Double mp Sim - Split by Sign of initial state"
"""